{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1QHcVQz4Gu7"
   },
   "source": [
    "In the first line we install Whisper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOvKw2K3kWqK"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FatSKi3YAQCx"
   },
   "source": [
    "Next we pull down some audio to transcribe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ2-aeeBAefF"
   },
   "source": [
    "Finally, we run Whisper! It may take a little time to get started, but soon the transcription should start to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uU1Bxv1y6zAx"
   },
   "outputs": [],
   "source": [
    "!whisper \"/content/morrison_297569.mp4\" --model small --language English "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOtm7PV0WwR3"
   },
   "source": [
    "## Checking Whisper's Work\n",
    "\n",
    "Whisper hasn't just produced text, it's given us time intervals where it believes that text occurred. In this section we'll read in Whisper's transcript, split up the audio according to Whisper's timestamps, and then print Whisper's text and play the corresponding audio. How well do they match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RITBdx3FXC5I"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMy80hPTgtZ3"
   },
   "source": [
    "Whisper's output is saved in `.vtt` format; we'll install `webvtt-py`, a package that can read that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKQ4gmnnBj85"
   },
   "outputs": [],
   "source": [
    "!pip install webvtt-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_C03ZyYWni9"
   },
   "outputs": [],
   "source": [
    "import webvtt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T515Pxovg2Ug"
   },
   "source": [
    "`librosa` is a library for reading and manipulating audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XJDcmz1YJ4g"
   },
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F12-3a4phEMi"
   },
   "source": [
    "We have two custom functions here, one to convert H:M:S timestamps into seconds, and another to trim out a chunk of audio corresponding to a particular `start` and `end` time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qopgBohKZ8vI"
   },
   "outputs": [],
   "source": [
    "def simple_hms(s):\n",
    "  h,m,sec = [float(x) for x in s.split(':')]\n",
    "  return 3600*h + 60*m + sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXBSOwKHbKfQ"
   },
   "outputs": [],
   "source": [
    "def trim_audio(row,audio,sample_rate):\n",
    "  t = np.arange(len(audio))\n",
    "  t = t/sample_rate\n",
    "  f = np.where( (t>=row.start_s) & (t<=row.end_s) )\n",
    "  return audio[f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zis6Xr1LhQeO"
   },
   "source": [
    "As promised, we use `webvtt` to read in the transcript and `librosa` to read in the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3p1-1svW7R8"
   },
   "outputs": [],
   "source": [
    "transcript = webvtt.read('/content/morrison_297569.vtt')\n",
    "audio,sample_rate = librosa.load('/content/morrison_297569.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgmVVTWbhYAN"
   },
   "source": [
    "For convenience we're going to set up a Pandas dataframe to store the various quantities we want to track. Each row will correspond to one segment of the Whisper transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUYKIyXAXYTO"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['start','end','text'])\n",
    "\n",
    "df['start'] = [x.start for x in transcript]\n",
    "df['end'] = [x.end for x in transcript]\n",
    "df['text'] = [x.text for x in transcript]\n",
    "df['start_s'] = df['start'].apply(simple_hms)\n",
    "df['end_s'] = df['end'].apply(simple_hms)\n",
    "df['audio'] = df.apply(trim_audio,axis=1,args=(audio,sample_rate))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k72S1VbUCa-B"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"morrison_297569.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "op8ojXOsCa_m"
   },
   "outputs": [],
   "source": [
    "#df.to_json(\"morrison_297569_main.json\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
