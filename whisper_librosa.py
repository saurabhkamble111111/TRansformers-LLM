# -*- coding: utf-8 -*-
"""Whisper_Librosa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-MuyedxHQDofLd_GsAf6yRkv7L7rJzcj
"""

from pydub import AudioSegment

# Load the MP4 file
mp4_path = "bates_121913_shy.mp4"
audio = AudioSegment.from_file(mp4_path, format="mp4")

# Export the audio as WAV
wav_path = "bates_121913_shy.wav"
audio.export(wav_path, format="wav")

!pip install pydub

import subprocess

mp4_path = "bates_121913_shy.mp4"
wav_path = "bates_121913_shy1.wav"

# Run the ffmpeg command as a subprocess
subprocess.run(["ffmpeg", "-i", mp4_path, wav_path])

!pip install git+https://github.com/openai/whisper.git



import whisper

model = whisper.load_model("base")

result = model.transcribe("/content/bates_121913_shy.wav", fp16=False, language='English')

print(result["text"])

text = result["text"]

text

import librosa
import nltk

# Load the audio file
audio_path = "/content/bates_121913_shy.wav"
y, sr = librosa.load(audio_path)

# Tokenize the string
tokens = nltk.word_tokenize(text)

# Get the duration of each token
token_durations = []
for token in tokens:
    # Compute the duration of the token in seconds
    token_duration = librosa.core.samples_to_time(len(token) * sr)
    token_durations.append(token_duration)

# Compute the timestamp of each token
timestamps = []
current_time = 0.0
for token_duration in token_durations:
    timestamps.append(current_time)
    current_time += token_duration

print("Token Timestamps:", timestamps)

import nltk
nltk.download('punkt')

import pandas as pd
import librosa
import numpy as np
import nltk

# Example input string
#input_string = "hello world how are you"

# Tokenize the input string
tokens = nltk.word_tokenize(text)

# Load audio file
audio_data, sampling_rate = librosa.load('/content/bates_121913_shy.wav')

# Convert audio data to floating-point format
audio_data_float = audio_data.astype(np.float32)

# Compute the duration of each token in seconds
token_durations = []
for token in tokens:
    # Compute the duration of the token in seconds
    token_duration = librosa.core.samples_to_time(len(token) * sampling_rate)
    token_durations.append(token_duration)

# Compute the timestamp of each token
timestamps = []
current_time = 0.0
for token_duration in token_durations:
    timestamps.append(current_time)
    current_time += token_duration
timestamps.append(current_time) # Add the end time of the last token

# Assign a default confidence score of 1.0 to each token
confidence_scores = [1.0] * len(tokens)

# Create a pandas dataframe with the token, start time, end time, and confidence score
df = pd.DataFrame({'Token': tokens, 'Start Time': timestamps[:-1], 'End Time': timestamps[1:], 'Confidence Score': confidence_scores})

# Print the resulting dataframe
print(df)

df.to_csv("WhisperOutput.csv")

df.to_json("WhisperOutput.json")









